import 'dotenv/config'
import express from 'express'
import cors from 'cors'
import { OllamaProvider } from './ollamaProvider.js'
import { ChatProvider } from './chatProvider.js'

const app = express()
const PORT = process.env.PORT || 3001

// ä¸­é–“ä»¶
app.use(cors())
app.use(express.json())

// åˆå§‹åŒ–æä¾›è€… - æ”¯æ´ç’°å¢ƒè®Šæ•¸è¨­å®š
const defaultApiUrl = process.env.OLLAMA_API_URL || 'http://localhost:11434'
const defaultApiKey = process.env.OLLAMA_API_KEY || ''
const ollamaProvider = new OllamaProvider(defaultApiUrl, defaultApiKey)
const chatProvider = new ChatProvider(ollamaProvider)

// å¥åº·æª¢æŸ¥ç«¯é»
app.get('/api/health', (req, res) => {
    res.json({ status: 'ok', timestamp: new Date().toISOString() })
})

// ç²å–é è¨­é…ç½®
app.get('/api/config', (req, res) => {
    res.json({
        apiUrl: defaultApiUrl,
        apiKey: defaultApiKey ? 'configured' : ''
    })
})

// ç²å–å¯ç”¨æ¨¡å‹ - OpenAI API ç›¸å®¹æ ¼å¼
app.get('/v1/models', async (req, res) => {
    try {
        const models = await ollamaProvider.getAvailableModels()

        // OpenAI API ç›¸å®¹çš„éŸ¿æ‡‰æ ¼å¼
        const openaiModels = models.map(model => ({
            id: model.name,
            object: 'model',
            created: Math.floor(Date.now() / 1000),
            owned_by: 'local'
        }))

        res.json({
            object: 'list',
            data: openaiModels
        })
    } catch (error) {
        console.error('Error fetching models:', error)
        res.status(500).json({
            error: {
                message: 'ç„¡æ³•ç²å–æ¨¡å‹åˆ—è¡¨',
                type: 'invalid_request_error'
            }
        })
    }
})

// ç²å–å¯ç”¨æ¨¡å‹ - æ”¯æŒè‡ªå®šç¾© API URL
app.get('/api/models', async (req, res) => {
    try {
        const apiUrl = req.query.apiUrl || 'http://localhost:11434'
        const dynamicProvider = new OllamaProvider(apiUrl)
        const models = await dynamicProvider.getAvailableModels()
        res.json({ models })
    } catch (error) {
        console.error('Error fetching models:', error)
        res.status(500).json({ error: 'ç„¡æ³•ç²å–æ¨¡å‹åˆ—è¡¨' })
    }
})

// èŠå¤©ç«¯é» - æ”¯æŒè‡ªå®šç¾© API URL å’Œ API Key
app.post('/api/chat', async (req, res) => {
    try {
        const { message, settings, history } = req.body

        if (!message) {
            return res.status(400).json({ error: 'æ¶ˆæ¯ä¸èƒ½ç‚ºç©º' })
        }

        // è¨­ç½®é è¨­è¨­å®š
        const chatSettings = {
            model: settings?.model || 'llama2',
            temperature: settings?.temperature || 0.7,
            maxTokens: settings?.maxTokens || 2048,
            systemPrompt: settings?.systemPrompt || 'ä½ æ˜¯ä¸€å€‹æœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚',
            apiUrl: settings?.apiUrl || 'http://localhost:11434',
            apiKey: settings?.apiKey || ''
        }

        // ä½¿ç”¨è‡ªå®šç¾© API URL å’Œ API Key çš„å‹•æ…‹æä¾›è€…
        const dynamicProvider = new OllamaProvider(chatSettings.apiUrl, chatSettings.apiKey)
        const dynamicChatProvider = new ChatProvider(dynamicProvider)

        // ç”Ÿæˆå›æ‡‰
        console.log('Generating response for message:', message.substring(0, 50))
        const response = await dynamicChatProvider.generateResponse({
            message,
            history: history || [],
            settings: chatSettings
        })

        console.log('Response generated:', response.substring(0, 50))
        res.json({ response })
    } catch (error) {
        console.error('Chat error:', error)
        console.error('Error stack:', error.stack)
        res.status(500).json({ error: 'è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤', details: error.message })
    }
})

// æµå¼èŠå¤©ç«¯é» - æ”¯æŒå¯¦æ™‚ä¸²æµå›æ‡‰
app.post('/api/chat/stream', async (req, res) => {
    try {
        const { message, settings, history } = req.body

        if (!message) {
            return res.status(400).json({ error: 'æ¶ˆæ¯ä¸èƒ½ç‚ºç©º' })
        }

        // è¨­ç½®æµå¼å›æ‡‰é ­
        res.setHeader('Content-Type', 'text/plain; charset=utf-8')
        res.setHeader('Cache-Control', 'no-cache')
        res.setHeader('Connection', 'keep-alive')

        // è¨­ç½®é è¨­è¨­å®š
        const chatSettings = {
            model: settings?.model || 'llama2',
            temperature: settings?.temperature || 0.7,
            maxTokens: settings?.maxTokens || 2048,
            systemPrompt: settings?.systemPrompt || 'ä½ æ˜¯ä¸€å€‹æœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚',
            apiUrl: settings?.apiUrl || 'http://localhost:11434',
            apiKey: settings?.apiKey || ''
        }

        // ä½¿ç”¨è‡ªå®šç¾© API URL å’Œ API Key çš„å‹•æ…‹æä¾›è€…
        const dynamicProvider = new OllamaProvider(chatSettings.apiUrl, chatSettings.apiKey)

        // ä½¿ç”¨ OllamaProvider çš„æµå¼ç”Ÿæˆæ–¹æ³•
        try {
            const streamGenerator = dynamicProvider.generateResponseStream({
                message,
                history: history || [],
                settings: chatSettings
            })

            for await (const chunk of streamGenerator) {
                console.log('Streaming chunk:', chunk)
                res.write(chunk)
            }

            console.log('Stream completed successfully')
            res.end()
        } catch (error) {
            console.error('Stream processing error:', error)
            if (!res.headersSent) {
                res.status(500).json({ error: 'æµå¼è™•ç†éŒ¯èª¤', details: error.message })
            } else {
                res.end()
            }
        }

    } catch (error) {
        console.error('Stream chat error:', error)
        if (!res.headersSent) {
            res.status(500).json({ error: 'è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤', details: error.message })
        } else {
            res.end()
        }
    }
})

// èŠå¤©æ­·å²ç«¯é»ï¼ˆå¯é¸åŠŸèƒ½ï¼‰
app.get('/api/history', (req, res) => {
    // é€™è£¡å¯ä»¥å¯¦ç¾å¾æ•¸æ“šåº«ç²å–èŠå¤©æ­·å²çš„åŠŸèƒ½
    // ç›®å‰è¿”å›ç©ºæ•¸çµ„ï¼Œå¯ä»¥å¾ŒçºŒæ“´å±•
    res.json({ history: [] })
})

// å…¨å±€éŒ¯èª¤è™•ç†
app.use((error, req, res, next) => {
    console.error('Global error:', error)
    res.status(500).json({
        error: 'æœå‹™å™¨å…§éƒ¨éŒ¯èª¤',
        message: process.env.NODE_ENV === 'development' ? error.message : 'è«‹ç¨å¾Œå†è©¦'
    })
})

// å•Ÿå‹•æœå‹™å™¨
app.listen(PORT, () => {
    console.log(`ğŸš€ Local LLM Chat Server é‹è¡Œåœ¨ http://localhost:${PORT}`)
    console.log(`ğŸ“ API ç«¯é»:`)
    console.log(`   - GET  /api/health     - å¥åº·æª¢æŸ¥`)
    console.log(`   - GET  /v1/models      - ç²å–æ¨¡å‹åˆ—è¡¨ (OpenAI æ ¼å¼)`)
    console.log(`   - GET  /api/models     - ç²å–æ¨¡å‹åˆ—è¡¨ (èˆŠæ ¼å¼)`)
    console.log(`   - POST /api/chat       - èŠå¤©`)
    console.log(`   - GET  /api/history    - èŠå¤©æ­·å²`)
    console.log(`ğŸ”§ é…ç½®:`)
    console.log(`   - Ollama API URL: ${defaultApiUrl}`)
    console.log(`   - API Key: ${defaultApiKey ? 'å·²è¨­å®š' : 'æœªè¨­å®š'}`)

    // æ¸¬è©¦ Ollama é€£æ¥
    ollamaProvider.checkConnection()
        .then(connected => {
            if (connected) {
                console.log('âœ… Ollama é€£æ¥æ­£å¸¸')
            } else {
                console.warn('âš ï¸  Ollama é€£æ¥å¤±æ•—ï¼Œè«‹ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œ')
            }
        })
        .catch(error => {
            console.warn('âš ï¸  æª¢æŸ¥ Ollama é€£æ¥æ™‚ç™¼ç”ŸéŒ¯èª¤:', error.message)
        })
})

export default app